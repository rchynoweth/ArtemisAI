{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b975075-169c-4984-9273-35c29f3d94f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# machine learning\n",
    "import mlflow\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# spark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b9a7d2-4254-49b8-ae54-54e7ad143a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# job parameter\n",
    "dbutils.widgets.text('schema_name', 'artemis')\n",
    "dbutils.widgets.text('max_datetime', '2025-03-31 00:00:00')\n",
    "# dbutils.widgets.text('trailing_n', '3')\n",
    "\n",
    "schema_name = dbutils.widgets.get('schema_name')\n",
    "max_datetime = dbutils.widgets.get('max_datetime')\n",
    "# trailing_n = int(dbutils.widgets.get('trailing_n'))\n",
    "# print(f\"---------> {trailing_n} | {max_datetime}\")\n",
    "\n",
    "\n",
    "# forecast variables\n",
    "interval_width = 0.8\n",
    "forecast_frequency = 'H'\n",
    "forecast_periods = 8\n",
    "include_history = True\n",
    "freq = 'H'\n",
    "group_cols = ['miner_id','seasonality_mode','changepoint_grid']\n",
    "\n",
    "\n",
    "# dataset variables\n",
    "# dt = datetime.fromisoformat(max_datetime)\n",
    "# min_date = dt - timedelta(days=trailing_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb67dfd6-3622-4ff7-badd-1e5d99851e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = '/Users/rchynoweth@invisocorp.com/exps/Forecast_Power_Usage'\n",
    "\n",
    "try:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print('Experiment created')\n",
    "except : \n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print('Experiment set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8109d3c1-024a-43a8-a08c-26df4b00155c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"use {schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df15cd8e-b3f9-4291-b9ad-7f452e447da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\n",
    "    f\"\"\"\n",
    "        select \n",
    "        time as ds\n",
    "        , miner_id\n",
    "        , kwh as y\n",
    "        from {schema_name}.miner_data\n",
    "    \"\"\"\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271457df-e408-4498-bda1-021c007f577f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "changepoint_grid = np.linspace(0.5, 0.95, num=12).tolist()  \n",
    "seasonality_modes = ['additive', 'multiplicative']\n",
    "\n",
    "# Create the cross product manually\n",
    "cross_product = [(c, s) for c in changepoint_grid for s in seasonality_modes]\n",
    "\n",
    "# Create a DataFrame from the cross product\n",
    "schema = StructType([\n",
    "    StructField(\"changepoint_grid\", DoubleType(), False),\n",
    "    StructField(\"seasonality_mode\", StringType(), False)\n",
    "])\n",
    "\n",
    "cross_df = spark.createDataFrame(cross_product, schema=schema)\n",
    "\n",
    "# Cross join your original df with the cross_df\n",
    "df = df.crossJoin(cross_df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee4d05c1-4516-4f08-8f3f-a3a0477fe1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Detailed Forecasts\n",
    "Using the provided job parameters, we will perform hyperparameter tuning for all miners to find the optimal `changepoint_prior_scale` and `seasonality_mode` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadeaedb-2eb3-4aae-b302-da1e43d3be70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_output_schema = StructType(\n",
    "    [\n",
    "        StructField(\"ds\", TimestampType(), True),\n",
    "        StructField(\"trend\", DoubleType(), True),\n",
    "        StructField(\"yhat_lower\", DoubleType(), True),\n",
    "        StructField(\"yhat_upper\", DoubleType(), True),\n",
    "        StructField(\"trend_lower\", DoubleType(), True),\n",
    "        StructField(\"trend_upper\", DoubleType(), True),\n",
    "        StructField(\"additive_terms\", DoubleType(), True),\n",
    "        StructField(\"additive_terms_lower\", DoubleType(), True),\n",
    "        StructField(\"additive_terms_upper\", DoubleType(), True),\n",
    "        StructField(\"daily\", DoubleType(), True),\n",
    "        StructField(\"daily_lower\", DoubleType(), True),\n",
    "        StructField(\"daily_upper\", DoubleType(), True),\n",
    "        StructField(\"weekly\", DoubleType(), True),\n",
    "        StructField(\"weekly_lower\", DoubleType(), True),\n",
    "        StructField(\"weekly_upper\", DoubleType(), True),\n",
    "        # StructField(\"hourly\", DoubleType(), True),\n",
    "        # StructField(\"hourly_lower\", DoubleType(), True),\n",
    "        # StructField(\"hourly_upper\", DoubleType(), True),\n",
    "        StructField(\"extra_regressors_additive\", DoubleType(), True),\n",
    "        StructField(\"extra_regressors_additive_lower\", DoubleType(), True),\n",
    "        StructField(\"extra_regressors_additive_upper\", DoubleType(), True),\n",
    "        # StructField(\"is_afternoon\", IntegerType(), True),\n",
    "        StructField(\"is_afternoon_lower\", DoubleType(), True),\n",
    "        StructField(\"is_afternoon_upper\", DoubleType(), True),\n",
    "        StructField(\"is_afternoon_x\", DoubleType(), True),\n",
    "        StructField(\"is_afternoon_y\", DoubleType(), True),\n",
    "        StructField(\"multiplicative_terms\", DoubleType(), True),\n",
    "        StructField(\"multiplicative_terms_lower\", DoubleType(), True),\n",
    "        StructField(\"multiplicative_terms_upper\", DoubleType(), True),\n",
    "        StructField(\"yhat\", DoubleType(), True),\n",
    "        StructField(\"miner_id\", IntegerType(), True),\n",
    "        StructField(\"changepoint_grid\", DoubleType(), True),\n",
    "        StructField(\"seasonality_mode\", StringType(), True),\n",
    "        StructField(\"y\", DoubleType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73533e4-ea8d-427e-a7bc-985444b0fea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def is_afternoon(ds):\n",
    "    dt = pd.to_datetime(ds)\n",
    "    if dt.hour >= 17 and dt.hour <= 23:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d094917e-982e-4ba0-9dd4-1730b6e50d18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def miner_forecast(pdf):\n",
    "    miner_id = pdf['miner_id'].iloc[0]\n",
    "    # pdf = (df.filter(col('miner_id')==10796964)\n",
    "    #     .filter(col('changepoint_grid')==0.80)\n",
    "    #     .filter(col('seasonality_mode')=='multiplicative')\n",
    "    #     .toPandas()\n",
    "    # )\n",
    "    # experiment_name = '/Users/rchynoweth@invisocorp.com/Forecast_Power_Usage'\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(nested=True, run_name=f\"Miner_{miner_id}\"):\n",
    "        # ensure proper datetime for filtering\n",
    "        pdf['ds'] = pd.to_datetime(pdf['ds'])\n",
    "        pdf['is_afternoon'] = pdf['ds'].apply(is_afternoon)\n",
    "\n",
    "        # make a copy and filter to the dates we want to use for forecasting\n",
    "        history_pd = pdf.dropna().copy()\n",
    "        history_pd = history_pd[history_pd['ds'] <= pd.to_datetime(max_datetime)]\n",
    "        # history_pd = history_pd[history_pd['ds'] >= pd.to_datetime(min_date)]\n",
    "\n",
    "        # find the extra columns that we want to keep\n",
    "        extra_cols = [col for col in history_pd.columns if col not in ['ds', 'y']]\n",
    "\n",
    "        # get the hyperparameter values from the dataframe\n",
    "        cp = pdf['changepoint_grid'].iloc[0]\n",
    "        mode = pdf['seasonality_mode'].iloc[0]\n",
    "\n",
    "        # create the model\n",
    "        model = Prophet(\n",
    "            interval_width=interval_width,\n",
    "            changepoint_prior_scale=cp,\n",
    "            seasonality_mode=mode\n",
    "        )\n",
    "        model.add_regressor('is_afternoon')\n",
    "        # model.add_seasonality(name='hourly', period=24, fourier_order=8)\n",
    "        # train model\n",
    "        model.fit(history_pd)\n",
    "\n",
    "        # Forecast data \n",
    "        future_pd = model.make_future_dataframe(\n",
    "            periods=forecast_periods, \n",
    "            freq=forecast_frequency, \n",
    "            include_history=include_history\n",
    "        )\n",
    "        future_pd['is_afternoon'] = future_pd['ds'].apply(is_afternoon)\n",
    "\n",
    "        forecast_pd = model.predict(future_pd)\n",
    "        # Add any extra columns back\n",
    "        for c in extra_cols:\n",
    "            forecast_pd[c] = history_pd[c].iloc[0]\n",
    "\n",
    "        forecast_pd = pd.merge(forecast_pd, pdf, on=['ds','miner_id', 'changepoint_grid', 'seasonality_mode'], how='left')\n",
    "\n",
    "        # Calculate evaluation on the forecasted points only \n",
    "        ## Filter both DataFrames first\n",
    "        forecast_filtered = forecast_pd[forecast_pd['ds'] > pd.to_datetime(max_datetime)]\n",
    "\n",
    "        ## Perform the join (e.g., inner join on 'ds')\n",
    "        y_true = forecast_filtered[['y']]\n",
    "        y_pred = forecast_filtered[['yhat']]\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        rmse_first_four = mean_squared_error(y_true.iloc[:4], y_pred.iloc[:4], squared=False)\n",
    "        match = ((y_true['y'] > 4.6) == (y_pred['yhat'] > 4.6)).astype(int)\n",
    "        accuracy = match.sum() / len(match)\n",
    "\n",
    "        # custom accuracy for when y is above 4.6\n",
    "        # Filter only the rows where y_true > 4.6\n",
    "        mask = y_true['y'] > 4.6\n",
    "        # Now check if y_pred is also > 4.6 in those cases\n",
    "        correct_predictions = (y_pred.loc[mask, 'yhat'] > 4.6).astype(int)\n",
    "        # Accuracy is sum of correct predictions divided by number of cases evaluated\n",
    "        accuracy46 = correct_predictions.sum() / len(correct_predictions) if len(correct_predictions) > 0 else 0\n",
    "\n",
    "        mlflow.log_param('changepoint_prior_scale', cp)\n",
    "        mlflow.log_param('seasonality_mode', mode)\n",
    "        mlflow.log_metric('rmse', rmse)\n",
    "        mlflow.log_metric('rmse_first_four', rmse_first_four)\n",
    "        mlflow.log_metric('accuracy', accuracy)\n",
    "        mlflow.log_metric('accuracy46', accuracy46)\n",
    "        mlflow.log_param(\"miner_id\", miner_id)\n",
    "        \n",
    "        # forecast_pd = forecast_pd.drop(columns=['is_afternoon'])\n",
    "\n",
    "\n",
    "    return forecast_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "026f0b7d-d644-45aa-aab6-57acf48428a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"ForecastAllMiners\") as parent_run:\n",
    "\n",
    "  out_df = (df\n",
    "            # .filter(col('ds')<max_datetime)\n",
    "            # .filter(col('ds')>min_date.isoformat())\n",
    "            .groupBy(*group_cols)\n",
    "            .applyInPandas(miner_forecast, schema=forecast_output_schema)\n",
    "            .withColumn(\"yhat_lower\", greatest(col(\"yhat_lower\"), lit(0)))\n",
    "            .withColumn(\"yhat_upper\", greatest(col(\"yhat_upper\"), lit(0)))\n",
    "            .withColumn('training_time', current_timestamp())\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965353f7-51c2-43cd-86cb-36530c2bfa56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_df.write.option(\"mergeSchema\", \"true\").mode('append').saveAsTable('artemis.forecasts_output_v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0016301-208e-4507-8af6-73dec761cd98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# mlflow\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Search for all runs\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"attributes.status = 'FINISHED'\"\n",
    "    # ,output_format=\"pandas\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00083e4e-0b0d-4e69-971e-6488deca8084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary = (runs_df[runs_df['start_time'] > '2025-05-04']\n",
    "           .groupby(['params.changepoint_prior_scale', 'params.seasonality_mode'])\n",
    "           .agg(\n",
    "               avg_rmse=('metrics.rmse', 'mean'),\n",
    "               std_rmse=('metrics.rmse', 'std'),\n",
    "               min_rmse=('metrics.rmse', 'min'),\n",
    "               max_rmse=('metrics.rmse', 'max'),\n",
    "               count=('metrics.rmse', 'count'),\n",
    "               avg_4_rmse=('metrics.rmse_first_four', 'mean'),\n",
    "               std_4_rmse=('metrics.rmse_first_four', 'std'),\n",
    "               min_4_rmse=('metrics.rmse_first_four', 'min'),\n",
    "               max_4_rmse=('metrics.rmse_first_four', 'max'),\n",
    "               avg_accuracy=('metrics.accuracy', 'mean'),\n",
    "               std_accuracy=('metrics.accuracy', 'std'),\n",
    "               min_accuracy=('metrics.accuracy', 'min'),\n",
    "               max_accuracy=('metrics.accuracy', 'max'),\n",
    "               avg_accuracy46=('metrics.accuracy46', 'mean'),\n",
    "               std_accuracy46=('metrics.accuracy46', 'std'),\n",
    "               min_accuracy46=('metrics.accuracy46', 'min'),\n",
    "               max_accuracy46=('metrics.accuracy46', 'max')\n",
    "           )\n",
    "           .reset_index()\n",
    "           .sort_values('avg_rmse')\n",
    ")\n",
    "\n",
    "display(summary)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Forecast_Power_Usage",
   "widgets": {
    "max_datetime": {
     "currentValue": "2025-03-31 00:00:00",
     "nuid": "faf2b8f4-f450-4c61-8ff1-ea1be1cc2722",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-03-31 00:00:00",
      "label": null,
      "name": "max_datetime",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-03-31 00:00:00",
      "label": null,
      "name": "max_datetime",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "artemis",
     "nuid": "0e2f4bf7-0d76-4662-b0da-995afcf1394a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "artemis",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "artemis",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
